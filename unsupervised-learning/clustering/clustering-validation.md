## Clustering Validation

The term **cluster validation **is used to design the procedure of evaluating the goodness of clustering algorithm results. This is important to avoid finding patterns in a random data, as well as, in the situation where you want to compare two clustering algorithms. Clustering validation considers 2 metrics: Compactness on how close the points are within cluster and Separability, how distinctly separable points are in different clusters.

Generally, clustering validation statistics can be categorized into 3 classes\(Charrad et al. 2014,Brock et al. \(2008\),Theodoridis and Koutroumbas \(2008\)\):

1. **Internal cluster validation**, which uses the internal information of the clustering process to evaluate the goodness of a clustering structure without reference to external information. It can be also used for estimating the number of clusters and the appropriate clustering algorithm without any external data.
2. **External cluster validation**, which consists in comparing the results of a cluster analysis to an externally known result, such as externally provided class labels. It measures the extent to which cluster labels match externally supplied class labels. Since we know the “true” cluster number in advance, this approach is mainly used for selecting the right clustering algorithm for a specific data set.
3. **Relative cluster validation**, which evaluates the clustering structure by varying different parameter values for the same algorithm \(e.g.,: varying the number of clusters k\). It’s generally used for determining the optimal number of clusters.

![](/assets/Screenshot 2019-08-10 at 5.20.45 PM.png)![](/assets/Screenshot 2019-08-10 at 5.23.09 PM.png)![](/assets/Screenshot 2019-08-10 at 5.24.46 PM.png)![](/assets/Screenshot 2019-08-10 at 5.27.43 PM.png)![](/assets/Screenshot 2019-08-10 at 6.36.16 PM.png)![](/assets/Screenshot 2019-08-10 at 6.37.21 PM.png)![](/assets/Screenshot 2019-08-10 at 6.38.43 PM.png)

Silhoutte coefficient should not be used for DBSCCAN as the score rewards compact, dense well separated clusters but does not take into coonsideration noise.![](/assets/Screenshot 2019-08-10 at 6.39.40 PM.png)

Silhoutte coefficient shortcomings![](/assets/Screenshot 2019-08-10 at 6.42.27 PM.png)

Validation for DBSCAN: Density Based Clustering Validation - [http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=83C3BD5E078B1444CB26E243975507E1?doi=10.1.1.707.9034&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=83C3BD5E078B1444CB26E243975507E1?doi=10.1.1.707.9034&rep=rep1&type=pdf)

Silhouette analysis can be used to study the separation distance between the resulting clusters. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters visually. This measure has a range of \[-1, 1\].

Silhouette coefficients \(as these values are referred to as\) near +1 indicate that the sample is far away from the neighboring clusters. A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters and negative values indicate that those samples might have been assigned to the wrong cluster.

**Cluster metaphor**._"I preferred this method because it constitutes clusters such \(or such a way\) which meets with my concept of a cluster in my particular project"_. Each clustering algorithm or subalgorithm/method implies its corresponding structure/build/shape of a cluster. In regard to hierarchical methods, I've observed this in one of points[here](https://stats.stackexchange.com/a/63549/3277), and also[here](https://stats.stackexchange.com/q/195446/3277). I.e. some methods give clusters that are prototypically "types", other give "circles \[by interest\]", still other "\[political\] platforms", "classes", "chains", etc. Select that method which cluster metaphor suits you. For example, if I see my customer segments as types - more or less spherical shapes with compaction\(s\) in the middle I'll choose Ward's linkage method or K-means, but never single linkage method, clearly. If I need a focal representative point I could use medoid method. If I need to screen points for them being core and peripheral representatives I could use DBSCAN approach.

1. **Data/method assumptions**._"I preferred this method because my data nature or format predispose to it"_. This important and vast point is also mentioned in my link above. Different algorithms/methods may require different kind of data for them or different proximity measure to be applied to the data, and vice versa, different data may require different methods. There are methods for quantitative and methods for qualitative data. Mixture quantitative + qualitative features dramatically narrows the scope of choice among methods. Ward's or[K-means](https://stats.stackexchange.com/q/81481/3277)are based - explicitly or implicitly - on \(squared\) euclidean distance proximity measure only and not on arbitrary measure. Binary data may call for special similarity measures which in turn will strongly question using some methods, for example Ward's or K-means, for them. Big data may need special algorithms or special implementations.

2. **Internal validity**._"I preferred this method because it gave me most clear-cut, tight-and-isolated clusters"_. Choose algorithm/method that shows the best results for your data from this point of view. The more tight, dense are clusters inside and the less density is outside of them \(or the wider apart are the clusters\) - the greater is the internal validity. Select and use appropriate_internal clustering criteria_\([which are plenty](https://stats.stackexchange.com/q/21807/3277)- Calinski-Harabasz, Silhouette, etc etc; sometimes also called "stopping rules"\) to assess it. \[Beware of overfitting: all clustering methods seek to maximize some version of internal validity11\(it's what clustering_is_about\), so high validity may be partly due to random peculiarity of the given dataset; having a test dataset is always beneficial.\]

3. **External validity**._"I preferred this method because it gave me clusters which differ by their background or clusters which match with the true ones I know"_. If a clustering partition presents clusters which are clearly different on some important background \(i.e. not participated in the cluster analysis\) characteristics then it is an asset for that method which produced the partition. Use any analysis which applies to check the difference; there also exist a number of useful_external clustering criteria_\(Rand, F-measure, etc etc\). Another variant of external validation case is when you somehow know the true clusters in your data \(know "ground truth"\), such as when you generated the clusters yourself. Then how accurately your clustering method is able to uncover the real clusters is the measure of external validity.

4. **Cross-validity**._"I preferred this method because it is giving me very similar clusters on equivalent samples of the data or extrapolates well onto such samples"_. There are various approaches and their hybrids, some more feasible with some clustering methods while others with other methods. Two main approaches are_stability_check and_generalizability_check. Checking stability of a clustering method, one randomly splits or resamples the data in partly intersecting or fully disjoint sets and does the clustering on each; then matches and compares the solutions wrt some emergent cluster characteristic \(for example, a cluster's central tendency location\) whether it is stable across the sets. Checking generalizability implies doing clustering on a train set and then using its emergent cluster characteristic or rule to assign objects of a test set, plus also doing clustering on the test set. The assignment result's and the clustering result's cluster memberships of the test set objects are compared then.

5. **Interpretation**._"I preferred this method because it gave me clusters which, explained, are most persuasive that there is meaning in the world"_. It's not statistical - it is your psychological validation. How meaningful are the results for you, the domain and, possibly audience/client. Choose method giving most interpretable, spicy results.

6. **Gregariousness**. Some researches regularly and all researches occasionally would say_"I preferred this method because it gave with my data similar results with a number of other methods among all those I probed"_. This is a heuristic but questionable strategy which assumes that there exist quite universal data or quite universal method.

Points 1 and 2 are theoretical and precede obtaining the result; exclusive relying on these points is the haughty, self-assured exploratory strategy. Points 3, 4 and 5 are empirical and follow the result; exclusive relying on these points is the fidgety, try-all-out exploratory strategy. Point 6 is creative which means that it denies any result in order to try to rejustify it. Point 7 is loyal mauvaise foi.

Points 3 through 7 can also be judges in your selection of the "best"**number of clusters**.

